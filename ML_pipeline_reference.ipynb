{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "c738bcac", "cell_type": "markdown", "source": "# \ud83d\udcd8 ML Pipeline Reference Notebook\n\nThis notebook is a **comprehensive end-to-end guide** to solving a text classification problem using:\n- `CountVectorizer` for feature extraction\n- `LogisticRegression` for classification\n- Basic feature engineering (e.g., question mark counts)\n- `hstack` to combine features\n- `Pipeline` and `ColumnTransformer` for clean code\n- All key metrics: Accuracy, Precision, Recall, F1\n\n\u2705 This is your go-to working version for interview preparation.\n", "metadata": {}}, {"id": "6b194aae", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \u2705 Step 1: Import Required Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom scipy.sparse import hstack\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\n", "outputs": []}, {"id": "2d5d38e3", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \u2705 Step 2: Create a Sample Dataset\ndata = pd.DataFrame({\n    'text': [\n        'You won\u2019t believe what happened next!',\n        'Mayor announces new housing plan',\n        'How to lose weight with no effort?',\n        'Breaking: Local team wins championship',\n        'This one trick will save you thousands?',\n        'The new update crashed my app!',\n        'Scientists discover new particle',\n        'Why is the app so slow?',\n        'App notifications stopped working',\n        'Weather report for the weekend'\n    ],\n    'label': [1, 0, 1, 0, 1, 1, 0, 1, 1, 0]\n})\n\n# Add a simple feature: number of question marks in each text\ndata['question_marks'] = data['text'].apply(lambda x: x.count('?'))\ndata", "outputs": []}, {"id": "f4ffa379", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \u2705 Step 3: Train/Test Split\nX = data[['text', 'question_marks']]\ny = data['label']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)", "outputs": []}, {"id": "92382724", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \u2705 Step 4: ColumnTransformer + Pipeline\n# CountVectorizer on 'text', passthrough on 'question_marks'\n\ntext_transform = CountVectorizer(stop_words='english')\nqmark_transform = FunctionTransformer(lambda x: x[['question_marks']], validate=False)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('text', text_transform, 'text'),\n        ('qmark', qmark_transform, ['question_marks'])\n    ]\n)\n\npipeline = Pipeline([\n    ('preprocessing', preprocessor),\n    ('classifier', LogisticRegression())\n])\n\n# Train pipeline\npipeline.fit(X_train, y_train)", "outputs": []}, {"id": "307c7a08", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \u2705 Step 5: Evaluate the Model\ny_pred = pipeline.predict(X_test)\n\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Precision:\", precision_score(y_test, y_pred))\nprint(\"Recall:\", recall_score(y_test, y_pred))\nprint(\"F1 Score:\", f1_score(y_test, y_pred))", "outputs": []}, {"id": "b135d85b", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# \u2705 Step 6: Inspect Model Coefficients (Text Only)\nfeature_names = pipeline.named_steps['preprocessing'].named_transformers_['text'].get_feature_names_out()\ncoefficients = pipeline.named_steps['classifier'].coef_[0][:len(feature_names)]\n\ntop_features = sorted(zip(feature_names, coefficients), key=lambda x: abs(x[1]), reverse=True)[:5]\nprint(\"Top 5 influential words:\")\nfor word, coef in top_features:\n    print(f\"{word}: {coef:.4f}\")", "outputs": []}]}